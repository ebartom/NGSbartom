Introduction to Ceto

Please see also the accompanying diagram CetoDiagram2.png.

The goal of Ceto is to create a modular framework that can be flexibly re-wired based on a list of input files, a text-based table describing the experimental design, and a series of parameters and binary flags passed in by the user.  Even the modules are modular.  For example, one pipeline module is “Align Reads”.  Activating this modules turns on a series of sub-modules, based on a decision tree and the user’s inputs.  RNA-seq, ChIP-seq or DNA-seq?  Paired end or Single End?  Stranded or Unstranded library?  The perl script that reads in this information uses it to write a series of shell scripts, one for each sample, and, if indicated, launch them on Northwestern’s high performance compute cluster, Quest.  Users can easily re-run analyses (or run them in parallel) specifying different input files, different experimental designs, or different parameters / flags.  Ceto outputs a log file, with a description of its analysis plan based on the inputs, and with status updates as the analysis plan is carried out.  This is essential for rigor and reproducibility.  All executables are explicitly referenced in the shell scripts with their path and version, along with a time-date stamp and the parameters and input files the executables were called with.  The heart of Ceto is the “buildPipelineShellScripts.pl” perl script, and this and all other in-house scripts used by Ceto are in a publicly available Github repository (https://github.com/ebartom/NGSbartom), providing transparency and version control, in the interest of rigor and reproducibility.

Modularity is a key concept in Ceto, and the origin of the system’s name, borrowed from the name of the Greek goddess matriarch of a family of hybrid monsters (including Hydra, Chimera, Sphinx, Cerberus, and the Gorgons).  Ceto uses a set of standardized bioinformatics file formats to interface between its modules.  These include the raw sequence data in fastq format, BAM files mapping the sequence data to genomic coordinates, browser extensible data (BED) files listing regions of interest in a reference genome, wiggle files listing the read coverage across the genomic coordinates, textual lists of gene IDs, and tables of read counts for paired lists of genomic features and samples.  The first four of these formats can be browsed in a genome browser, to allow scientists to interactively explore their data, in the context of the genome and any publicly available data that can be similarly loaded into a genome browser.   These file formats were chosen because of their status as standardized file formats across computational biology and bioinformatics.  In addition to the pipeline modules, which can be linked together to do primary analysis on standardized files, there are also in-house scripts which may or may not be part of a pipeline, and which convert between file formats and provide different kinds of plots and visualizations of these file types either in isolation or combination.  These are part of Ceto’s Toolbox, and can be readily combined (along with open source tools such as aligners and peak callers) to extend existing modules or create new ones.  

Input files for Ceto can be sequenced in house, but they can also be downloaded from public repositories such as the Gene Expression Omnibus (GEO) and the Sequence Read Archive (SRA).  This facilitates easy comparison and integration of sequence data from different sources.  

Once a standard set of parameters and flags has been determined (a standard walk through the decision tree, given a particular type of input files), the resulting pipeline can be very efficiently run for all new sequencing data, so that investigators receive initial analysis results very quickly.  Previously in the Shilatifard lab, this initial analysis typically took about six weeks.  With Ceto, investigators receive preliminary results within 12-24 hours of the completion of a sequencing run.  At that point, they can explore the data interactively and independently, and see whether it is generally consistent or inconsistent with prior hypotheses.  We then sit down together and discuss which experiments, in silico, in vivo, or in vitro, might give further insight into the data.  For example, an initial survey of the data might indicate that a particular perturbation reduces ChIP-seq signal in promoters.  I can re-format that question as follows, “Given a set of genomic intervals defined in a BED format, and a set of wiggle files with read coverage information, plot the average signal across these genomic intervals for all wiggle files.”  I can readily solve this generalized problem in R, and the resulting Rscript can be added to Ceto’s Toolbox, and later used to address a question that at first glance might be unrelated.  For example, does average RNA-seq coverage at first exons change in response to perturbations in the transcription elongation machinery?  

Each script in the toolbox is written to address a particular research question, but once they are written they can readily be re-used to test new hypotheses.  In this way, modularity facilitates not only the standardized portions of Ceto, but the downstream customized analyses as well.  Ceto’s modular design also makes it easy to swap one analysis tool for another, or run several in parallel, in order to test new methods and continually innovate and improve.  By integrating more types of analysis modules into Ceto, I am able to spend more time working with students and postdocs to discuss and develop additional approaches for data analysis, allowing constant innovation as research projects develop.  

Elizabeth Bartom, PhD
Assistant Professor
Department of Biochemistry and Molecular Genetics
Feinberg School of Medicine
Northwestern University
email: ebartom@northwestern.edu
